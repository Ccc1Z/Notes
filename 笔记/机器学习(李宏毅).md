# 概述
* http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html
## 一些基本概念
* 机器学习就是自动找函数
* Regression:输出是一个数值
* Classification:
    * Binary Classification:输出是Yes or NO
    * Multi-class Classification:输出是Class1,Class2,...Class N(让机器做选择,从N个CLASS中做出一个选择)
* Generation:输出时有结构的复杂东西(文句、图片)->让机器创造东西(翻译、画图)
## 怎样告诉机器找怎样的函数
* Supervised Learning(监督学习):
    * 需要Labeled Data:告诉机器理想的输是什么样的
    * 函数式的Loss:评价输出的好坏(越小越好)->机器会自动找出Loss最低的f()
* Reinforcement Learning(强化学习):
    * AlphGo:通过不停的下棋来产生输或赢的Reward,通过Reward来指导机器的学习。
* Unsupervised Learning(Auto-encoder):在不给标注的情况下,机器能学到什么程度。 
* NetWork Architecture就是告诉机器函数的搜寻范围
* 函数寻找方法:Gradient Descent(梯度下降)等等



#

# Regression(回归)
## 概念及例子
* 机器学习就是自动找函数
* 回归是f(input)->数
* 例子:
    * f(股市历史情况) = 明天的股市情况
    * f(使用者A 商品B) = 购买可能性
#

Step1:Model(Fuction)
```math
y=b+\sum w_ix_i

input x:feature

wi:weight

b:bias
```
Step2:Goodness of Function
- Loss function L(Fuction中的Function):
    - Input:a function
    - output:how bad it is
```math
L(f) = L(w,b)

L(w,b)=\sum_{i=1}(\hat{y^n} - (b+w*\hat{x^n)})^2


衡量一组参数的好坏
```
Step3:Pick the Best Function
```math
f^* = \arg minL(f)

w^*,b^* =\arg minL(w,b)=\arg min \sum(\hat y^n - (b+w*x^n))^2

```
-->Gradient Descent(前提是L是可微的)  
- Consider loss function L(w) with one parameter w:
```math
w^* = \arg minL(w)
```
- 随机选取一个初始value w0;
- 计算dl/dw|w=w0
    - 如果斜率是负的,则增加w的值
    - 如果斜率是正德,则减少w的值
```math
(如果是两个参数,则分别求偏微分)

移动的步长=-\eta*\dfrac{dL}{dW}|_{w=w_0}

\eta ->"learning-rate"
```

- 重复上述步骤w0->w1    

        线性回归没有局部最小值,只有全局最小
        
        Overfitting(过拟合):在训练上得到很好的结果但是在测试集上得不到好结果。(Module不是越复杂越好)

## Basic Concept

### error的来源

- error due to "bias"
- error due to "variance"

